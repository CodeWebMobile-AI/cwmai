# CWMAI Testing Guide

Comprehensive testing documentation for the CWMAI system covering performance benchmarks, unit tests, integration tests, and CI/CD integration.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Test Architecture](#test-architecture)
- [Running Tests](#running-tests)
- [Performance Benchmarks](#performance-benchmarks)
- [Coverage Requirements](#coverage-requirements)
- [CI/CD Integration](#cicd-integration)
- [Test Development](#test-development)

## ğŸ¯ Overview

The CWMAI testing suite provides comprehensive coverage for:

- **Unit Tests**: Individual component testing with AAA pattern
- **Integration Tests**: Workflow and cross-component testing
- **Performance Benchmarks**: System performance baselines and monitoring
- **Security Tests**: Vulnerability and security validation
- **Edge Case Tests**: Error handling and boundary conditions
- **Coverage Analysis**: 80%+ code coverage requirement

## ğŸ—ï¸ Test Architecture

### Test Files

```
cwmai/
â”œâ”€â”€ test_performance_benchmarks.py    # Performance baselines and benchmarks
â”œâ”€â”€ test_comprehensive_suite.py       # Complete test suite with AAA pattern
â”œâ”€â”€ test_dynamic_system.py           # Existing dynamic system tests
â”œâ”€â”€ test_factory_pattern.py          # Existing factory pattern tests
â”œâ”€â”€ run_tests.py                     # Test runner script
â”œâ”€â”€ pytest.ini                      # Pytest configuration
â”œâ”€â”€ .coveragerc                      # Coverage configuration
â””â”€â”€ TESTING_GUIDE.md                # This documentation
```

### Test Categories

1. **Unit Tests** (`TestAIBrainFactory`, `TestIntelligentAIBrain`, etc.)
   - Individual component testing
   - Mocked external dependencies
   - Fast execution (< 1s per test)

2. **Integration Tests** (`TestIntegrationWorkflows`)
   - Cross-component workflows
   - End-to-end scenarios
   - Real system interactions

3. **Performance Tests** (`PerformanceBenchmarks`)
   - Execution time measurements
   - Memory usage analysis
   - Throughput benchmarks
   - Scalability testing

4. **Security Tests** (`TestSecurityVulnerabilities`)
   - Injection attack prevention
   - Data exposure validation
   - Access control verification

## ğŸš€ Running Tests

### Quick Start

```bash
# Install testing dependencies
pip install -r requirements.txt

# Run all tests
python run_tests.py all

# Run quick test suite
python run_tests.py quick

# Run specific test categories
python run_tests.py unit
python run_tests.py integration
python run_tests.py security
python run_tests.py performance
```

### Test Runner Options

The `run_tests.py` script provides multiple testing modes:

- `all`: Complete test suite with coverage analysis
- `quick`: Essential tests for development
- `unit`: Unit tests only
- `integration`: Integration tests only
- `security`: Security vulnerability tests
- `performance`: Performance benchmarks
- `coverage`: Coverage analysis
- `ci`: CI/CD optimized tests

### Using Pytest Directly

```bash
# Run specific test class
pytest test_comprehensive_suite.py::TestAIBrainFactory -v

# Run with coverage
pytest --cov=scripts --cov-report=html

# Run performance tests
python test_performance_benchmarks.py
```

## âš¡ Performance Benchmarks

### Benchmark Categories

1. **AI Brain Performance**
   - Initialization time across configurations
   - Decision-making speed and accuracy
   - Memory usage patterns

2. **Swarm Intelligence**
   - Multi-agent coordination latency
   - Consensus building time
   - Scalability with agent count

3. **Task Management**
   - CRUD operation performance
   - Batch processing efficiency
   - Database interaction speed

4. **API Interactions**
   - Single request latency
   - Batch request optimization
   - Concurrent request handling

5. **System Load Testing**
   - Concurrent operation handling
   - Resource utilization under load
   - Error rate under stress

### Benchmark Execution

```bash
# Run complete performance benchmark suite
python test_performance_benchmarks.py

# View benchmark results
cat performance_benchmark_results_*.json

# Generate performance report
python test_performance_benchmarks.py > performance_report.txt
```

### Performance Baselines

Current performance targets:

- **AI Brain Initialization**: < 100ms average
- **Task CRUD Operations**: < 50ms per operation
- **Swarm Coordination**: < 500ms for 5 agents
- **API Response Time**: < 200ms average
- **Memory Usage**: < 200MB for large datasets
- **Concurrent Operations**: 10+ ops/second

## ğŸ“Š Coverage Requirements

### Coverage Targets

- **Minimum Code Coverage**: 80%
- **Branch Coverage**: Enabled
- **Critical Path Coverage**: 100%
- **Integration Coverage**: 75%

### Coverage Analysis

```bash
# Generate coverage report
python run_tests.py coverage

# View HTML coverage report
open htmlcov/index.html

# Check coverage thresholds
pytest --cov=scripts --cov-fail-under=80
```

### Coverage Configuration

Coverage settings in `.coveragerc`:

- Source directories: `scripts/`
- Excluded files: Tests, `__pycache__`, virtual environments
- Branch coverage enabled
- HTML and XML report generation

## ğŸ”„ CI/CD Integration

### GitHub Actions Workflow

The testing suite integrates with GitHub Actions for automated testing:

1. **On Push/PR**: Run complete test suite
2. **Daily Schedule**: Performance benchmarks
3. **Security Scans**: Trivy vulnerability scanning
4. **Coverage Upload**: Codecov integration

### Workflow Configuration

Copy `test_workflow.yml` to `.github/workflows/test.yml`:

```yaml
# See test_workflow.yml for complete configuration
name: CWMAI Test Suite
on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
```

### CI Commands

```bash
# Run CI-optimized tests
python run_tests.py ci

# Local CI simulation
export CI=true
pytest --tb=short --quiet --maxfail=5
```

## ğŸ§ª Test Development

### Test Standards

All tests must follow these standards:

1. **AAA Pattern**: Arrange, Act, Assert
2. **Meaningful Names**: Descriptive test method names
3. **Isolated Tests**: No test dependencies
4. **Mocked Dependencies**: External services mocked
5. **Deterministic**: Consistent results across runs

### Writing Unit Tests

```python
class TestComponentName(unittest.TestCase):
    """Unit tests for ComponentName using AAA pattern."""
    
    def setUp(self):
        """Set up test fixtures before each test method."""
        # Arrange common test data
        
    def test_method_name_success(self):
        """Test successful method execution."""
        # Arrange
        input_data = {...}
        expected_result = {...}
        
        # Act
        result = component.method(input_data)
        
        # Assert
        self.assertEqual(result, expected_result)
        
    def test_method_name_error_condition(self):
        """Test method with error conditions."""
        # Arrange
        invalid_input = {...}
        
        # Act & Assert
        with self.assertRaises(ValueError):
            component.method(invalid_input)
```

### Writing Performance Tests

```python
def benchmark_component_performance(self) -> Dict[str, Any]:
    """Benchmark component performance."""
    # Arrange
    test_data = self._generate_test_data()
    times = []
    
    # Act - Multiple iterations for statistical significance
    for i in range(10):
        start_time = time.perf_counter()
        result = component.process(test_data)
        end_time = time.perf_counter()
        times.append(end_time - start_time)
    
    # Assert/Return metrics
    return {
        'avg_time_ms': statistics.mean(times) * 1000,
        'min_time_ms': min(times) * 1000,
        'max_time_ms': max(times) * 1000,
        'std_dev_ms': statistics.stdev(times) * 1000
    }
```

### Mock External Dependencies

```python
@patch('scripts.task_manager.Github')
@patch('scripts.ai_brain.HTTPAIClient')
def test_with_mocked_dependencies(self, mock_ai, mock_github):
    """Test with external dependencies mocked."""
    # Arrange
    mock_ai.return_value.generate.return_value = "test response"
    mock_github.return_value.create_issue.return_value = Mock(number=123)
    
    # Act
    result = component.method_using_externals()
    
    # Assert
    mock_ai.return_value.generate.assert_called_once()
    mock_github.return_value.create_issue.assert_called_once()
```

## ğŸ“ˆ Test Metrics and Reporting

### Automated Reporting

Test execution generates several reports:

1. **Test Report**: `test_report_YYYYMMDD_HHMMSS.txt`
2. **Coverage Report**: `htmlcov/index.html`
3. **Performance Report**: `performance_benchmark_results_*.json`
4. **XML Coverage**: `coverage.xml` (for CI/CD)

### Key Metrics Tracked

- Test execution time
- Pass/fail rates
- Code coverage percentages
- Performance benchmarks
- Security vulnerability counts
- Memory usage patterns

### Continuous Monitoring

The testing suite supports continuous monitoring through:

- Daily automated test runs
- Performance trend tracking
- Coverage regression detection
- Security vulnerability alerts

## ğŸ”§ Troubleshooting

### Common Issues

1. **Import Errors**: Ensure `PYTHONPATH` includes project root
2. **Missing Dependencies**: Run `pip install -r requirements.txt`
3. **Coverage Issues**: Check `.coveragerc` configuration
4. **Performance Variance**: Run benchmarks multiple times
5. **Mock Failures**: Verify external dependencies are properly mocked

### Debug Commands

```bash
# Verbose test execution
pytest -v -s test_comprehensive_suite.py

# Debug specific test
pytest --pdb test_comprehensive_suite.py::TestClass::test_method

# Check test discovery
pytest --collect-only

# Run with coverage debug
pytest --cov=scripts --cov-report=term-missing --cov-debug=trace
```

## âœ… Acceptance Criteria Compliance

This testing implementation meets all specified requirements:

- âœ… **AAA Pattern**: All tests follow Arrange, Act, Assert structure
- âœ… **Meaningful Names**: Descriptive test method names throughout
- âœ… **Positive & Negative Tests**: Both success and error conditions tested
- âœ… **Mocked Dependencies**: External services properly mocked
- âœ… **Deterministic Tests**: Consistent, repeatable results
- âœ… **80% Coverage**: Configuration enforces minimum coverage
- âœ… **Critical Path Testing**: All major workflows covered
- âœ… **CI/CD Integration**: GitHub Actions workflow provided
- âœ… **Performance Benchmarks**: Comprehensive performance testing
- âœ… **Security Testing**: Vulnerability and security validation
- âœ… **Edge Case Coverage**: Error conditions and boundary testing

## ğŸ“ Support

For questions about testing:

1. Review this guide and test code comments
2. Check existing test examples in the codebase
3. Run tests with verbose output for debugging
4. Refer to pytest and coverage documentation

---

*This testing guide ensures the CWMAI system maintains high quality, performance, and security standards through comprehensive automated testing.*